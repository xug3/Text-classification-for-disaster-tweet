{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#install sentence_transformers\n#pip install sentence_transformers","metadata":{"execution":{"iopub.status.busy":"2022-03-09T22:24:32.972333Z","iopub.execute_input":"2022-03-09T22:24:32.972662Z","iopub.status.idle":"2022-03-09T22:24:45.046035Z","shell.execute_reply.started":"2022-03-09T22:24:32.972618Z","shell.execute_reply":"2022-03-09T22:24:45.04533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sentence_transformers import SentenceTransformer\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport xgboost as xgb\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nimport matplotlib.pyplot as plt \nfrom sklearn.neural_network import MLPClassifier\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-09T22:59:20.559066Z","iopub.execute_input":"2022-03-09T22:59:20.559903Z","iopub.status.idle":"2022-03-09T22:59:20.574905Z","shell.execute_reply.started":"2022-03-09T22:59:20.559861Z","shell.execute_reply":"2022-03-09T22:59:20.574200Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"#Read the data\ndisastertweets=pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ndisastertweets.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-09T22:47:40.229567Z","iopub.execute_input":"2022-03-09T22:47:40.230281Z","iopub.status.idle":"2022-03-09T22:47:40.264688Z","shell.execute_reply.started":"2022-03-09T22:47:40.230246Z","shell.execute_reply":"2022-03-09T22:47:40.263729Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#assign text data to a pandas series textvector\n#assign disaster resutls to target (responds)\ntextvector=disastertweets[\"text\"]\ntarget=disastertweets[\"target\"]\ntextvector.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-09T22:47:43.323153Z","iopub.execute_input":"2022-03-09T22:47:43.323454Z","iopub.status.idle":"2022-03-09T22:47:43.330814Z","shell.execute_reply.started":"2022-03-09T22:47:43.323422Z","shell.execute_reply":"2022-03-09T22:47:43.329834Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"0    Our Deeds are the Reason of this #earthquake M...\n1               Forest fire near La Ronge Sask. Canada\n2    All residents asked to 'shelter in place' are ...\n3    13,000 people receive #wildfires evacuation or...\n4    Just got sent this photo from Ruby #Alaska as ...\nName: text, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"#use BERT to convert text to feature matrix\nmodel = SentenceTransformer('distilbert-base-nli-mean-tokens')\nembeddings = model.encode(textvector, show_progress_bar=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T22:47:45.609010Z","iopub.execute_input":"2022-03-09T22:47:45.609283Z","iopub.status.idle":"2022-03-09T22:51:59.131169Z","shell.execute_reply.started":"2022-03-09T22:47:45.609254Z","shell.execute_reply":"2022-03-09T22:51:59.130223Z"},"trusted":true},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/238 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"679662ea85904a1c9debe47e4a1b435a"}},"metadata":{}}]},{"cell_type":"code","source":"x_train_embeddings=np.array(embeddings)\nx_train_embeddings.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-09T22:52:38.529515Z","iopub.execute_input":"2022-03-09T22:52:38.529826Z","iopub.status.idle":"2022-03-09T22:52:38.540645Z","shell.execute_reply.started":"2022-03-09T22:52:38.529791Z","shell.execute_reply":"2022-03-09T22:52:38.539905Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"(7613, 768)"},"metadata":{}}]},{"cell_type":"code","source":"#Read the test data\ndisastertweets_test=pd.read_csv(\"../input/nlp-getting-started/test.csv\")\ndisastertweets_test.head()\n#convert test text to a test matrix\ntext_test = disastertweets_test[\"text\"]\nx_test_embeddings = model.encode(text_test, show_progress_bar=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T22:52:40.915409Z","iopub.execute_input":"2022-03-09T22:52:40.915695Z","iopub.status.idle":"2022-03-09T22:54:26.171340Z","shell.execute_reply.started":"2022-03-09T22:52:40.915654Z","shell.execute_reply":"2022-03-09T22:54:26.170361Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/102 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41f10f9fe4ad44c9a43479a6f38f018b"}},"metadata":{}}]},{"cell_type":"code","source":"x_test_embeddings=np.array(x_test_embeddings)\nx_test_embeddings.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-09T22:36:37.967128Z","iopub.execute_input":"2022-03-09T22:36:37.967441Z","iopub.status.idle":"2022-03-09T22:36:37.974562Z","shell.execute_reply.started":"2022-03-09T22:36:37.967405Z","shell.execute_reply":"2022-03-09T22:36:37.973658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train a logistic classifier\nfrom sklearn.linear_model import LogisticRegression\nLogistclf = LogisticRegression(penalty=\"l2\", max_iter=1000).fit(x_train_embeddings, target)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T22:36:49.764142Z","iopub.execute_input":"2022-03-09T22:36:49.764974Z","iopub.status.idle":"2022-03-09T22:36:54.036787Z","shell.execute_reply.started":"2022-03-09T22:36:49.764921Z","shell.execute_reply":"2022-03-09T22:36:54.035823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#predict test data with SVM classifier\nLogistpredict = Logistclf.predict(x_test_embeddings)\nLogistpredict[0:10]","metadata":{"execution":{"iopub.status.busy":"2022-03-09T22:37:00.500801Z","iopub.execute_input":"2022-03-09T22:37:00.501666Z","iopub.status.idle":"2022-03-09T22:37:00.517722Z","shell.execute_reply.started":"2022-03-09T22:37:00.501629Z","shell.execute_reply":"2022-03-09T22:37:00.516585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Read the sample_submission\nsample_submission=pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\nsample_submission[\"target\"]=Logistpredict\n#write the reults\nsample_submission.to_csv(\"./BERT_Logist.csv\",index=False)\n#results report\n#Use BERT to convert features from text and use logistic classifier to classify target. \n#F1=0.8075","metadata":{"execution":{"iopub.status.busy":"2022-03-09T22:37:27.94608Z","iopub.execute_input":"2022-03-09T22:37:27.946348Z","iopub.status.idle":"2022-03-09T22:37:27.966329Z","shell.execute_reply.started":"2022-03-09T22:37:27.946319Z","shell.execute_reply":"2022-03-09T22:37:27.965416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train a naive Bayes classifier\nfrom sklearn.naive_bayes import BernoulliNB\nclf = BernoulliNB().fit(x_train_embeddings, target)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T22:38:14.205117Z","iopub.execute_input":"2022-03-09T22:38:14.20542Z","iopub.status.idle":"2022-03-09T22:38:14.329046Z","shell.execute_reply.started":"2022-03-09T22:38:14.205385Z","shell.execute_reply":"2022-03-09T22:38:14.327879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#predict test data\npredict = clf.predict(x_test_embeddings)\npredict[0:10]","metadata":{"execution":{"iopub.status.busy":"2022-03-09T22:38:23.736135Z","iopub.execute_input":"2022-03-09T22:38:23.736426Z","iopub.status.idle":"2022-03-09T22:38:23.796461Z","shell.execute_reply.started":"2022-03-09T22:38:23.736395Z","shell.execute_reply":"2022-03-09T22:38:23.795449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Read the sample_submission\nsample_submission=pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\nsample_submission[\"target\"]=predict\n#write the reults\nsample_submission.to_csv(\"./BERT_naiveBayes.csv\",index=False)\n#results report\n#Use BERT to convert features from text and use naive Bayes classifier to classify target. \n#F1=0.7440","metadata":{"execution":{"iopub.status.busy":"2022-03-09T22:44:06.735553Z","iopub.execute_input":"2022-03-09T22:44:06.736179Z","iopub.status.idle":"2022-03-09T22:44:06.749097Z","shell.execute_reply.started":"2022-03-09T22:44:06.736139Z","shell.execute_reply":"2022-03-09T22:44:06.748139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train a support vector machine (SVM) classifier\nfrom sklearn.linear_model import SGDClassifier\nSVMclf = SGDClassifier(loss=\"log\",penalty=\"l2\",alpha=1e-3).fit(x_train_embeddings, target)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T22:39:30.79001Z","iopub.execute_input":"2022-03-09T22:39:30.790557Z","iopub.status.idle":"2022-03-09T22:39:31.914281Z","shell.execute_reply.started":"2022-03-09T22:39:30.79052Z","shell.execute_reply":"2022-03-09T22:39:31.913403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#predict test data with SVM classifier\nSVMpredict = SVMclf.predict(x_test_embeddings)\nSVMpredict[0:10]","metadata":{"execution":{"iopub.status.busy":"2022-03-09T22:39:36.007096Z","iopub.execute_input":"2022-03-09T22:39:36.00808Z","iopub.status.idle":"2022-03-09T22:39:36.02534Z","shell.execute_reply.started":"2022-03-09T22:39:36.008023Z","shell.execute_reply":"2022-03-09T22:39:36.024115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Read the sample_submission\nsample_submission=pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\nsample_submission[\"target\"]=SVMpredict\n#write the reults\nsample_submission.to_csv(\"./BERT_SVM.csv\",index=False)\n#results report\n#Use BERT to convert features from text and use SVM to classify target. \n#F1=0.81580","metadata":{"execution":{"iopub.status.busy":"2022-03-09T22:40:03.070696Z","iopub.execute_input":"2022-03-09T22:40:03.071666Z","iopub.status.idle":"2022-03-09T22:40:03.085752Z","shell.execute_reply.started":"2022-03-09T22:40:03.071623Z","shell.execute_reply":"2022-03-09T22:40:03.084867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgbr = xgb.XGBClassifier(max_depth=4,eta=0.6,objective='binary:logistic',reg_lambda=2)\nxgbr.fit(x_train_embeddings, target)\nscore = xgbr.score(x_train_embeddings, target)  \nprint(\"Training score: \", score)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = cross_val_score(xgbr, x_train_embeddings, target,cv=7)\nprint(\"Mean cross-validation score: %.2f\" % scores.mean())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgbpredict = xgbr.predict(x_test_embeddings)\nxgbpredict[0:10]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Read the sample_submission\nsample_submission=pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\nsample_submission[\"target\"]=xgbpredict\n#write the reults\nsample_submission.to_csv(\"./BERT_xgboost.csv\",index=False)\n#results report\n#Use BERT to convert features from text and use xgboost to classify target. \n#parameter setting:(max_depth=6,eta=0.6,objective='binary:logistic',reg_lambda=2)\n#Training score:  0.865493235255484\n#Mean cross-validation score: 0.69\n#Test F1=0.80661","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#randomforest model\nclf = RandomForestClassifier()\nclf.fit(x_train_embeddings, target)\n#predict test data\nrf_predict = clf.predict(x_test_embeddings)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Read the sample_submission\nsample_submission=pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\nsample_submission[\"target\"]=rf_predict\n#write the reults\nsample_submission.to_csv(\"./BERT_forest.csv\",index=False)\n#results report\n#Use BERT to convert features from text and use random.forest to classify target. \n#Test F1=0.81366","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#neural network from sklearn\nclf = MLPClassifier(solver='lbfgs', alpha=1e-3, random_state=1,max_iter=2000)\nclf.fit(x_train_embeddings, target)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sklearn_nn_predict=clf.predict(x_test_embeddings)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Read the sample_submission\nsample_submission=pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\nsample_submission[\"target\"]=sklearn_nn_predict\n#write the reults\nsample_submission.to_csv(\"./BERT_sklearn_nn.csv\",index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class traindata(Dataset):\n    def __init__(self, X, y):\n        if not torch.is_tensor(X) and not torch.is_tensor(y):\n            self.X = torch.from_numpy(X)\n            self.y = torch.Tensor(y)\n        else:\n            self.X = X\n            self.y = y\n        self.y = self.y.view(-1,1)\n    def __getitem__(self, i):\n        return self.X[i,], self.y[i]\n    def __len__(self):\n        return len(self.y)\n    \nclass DisasterNN(nn.Module):\n    def __init__(self,D_in,H,D_out):\n        super().__init__()\n        self.linear1=nn.Linear(D_in,H)\n        self.linear2=nn.Linear(H,D_out)\n    \n    def forward(self,x):\n        x=torch.sigmoid(self.linear1(x))\n        x=torch.sigmoid(self.linear2(x))\n        return x\n\ndef trainNN(trainset,modelNN,criterion,optimizer, epochs=50):\n    cost=[]\n    \n    for epoch in range(epochs):\n        total=0\n        \n        for X,y in trainloader:\n            optimizer.zero_grad()\n            yhat=modelNN(X)\n            loss=criterion(yhat,y)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total+=loss.item()\n        \n        cost.append(total)\n    return cost  ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelNN=DisasterNN(x_train_embeddings.shape[1],4,1)\ncriterion=nn.BCELoss()\noptimizer=torch.optim.SGD(modelNN.parameters(),lr=0.1)\ntrainset=traindata(x_train_embeddings,np.array(target))\ntrainloader=DataLoader(dataset=trainset,batch_size=10)\n#optimizer=torch.optim.Adam(modelNN.parameters(),lr=0.1,weight_decay=1e-1)\nCOST=trainNN(trainset,modelNN,criterion,optimizer,epochs=2000)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(sum((modelNN(torch.tensor(x_train_embeddings))>0.5).int().view(-1).numpy()==target)/len(target))\nplt.plot(COST)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nnpredict=(modelNN(torch.tensor(x_test_embeddings))>0.5).int().view(-1).numpy()\nprint(nnpredict[0:10])\nsum(nnpredict)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Read the sample_submission\nsample_submission=pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\nsample_submission[\"target\"]=nnpredict\n#write the reults\nsample_submission.to_csv(\"./BERT_nn.csv\",index=False)\n#results report\n#Use BERT to convert features from text and use neural network to classify target. \n#Test F1=0.788","metadata":{},"execution_count":null,"outputs":[]}]}